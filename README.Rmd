---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

```{r, echo = FALSE}
# options(ask.cache = system.file("cache/readme", package = "ask"))
```

# ask

{ask} is designed to ask R anything with minimal effort. This includes changing
code in place and sending commands to the terminal.
To do so we use natural language (typed or spoken), and simple well 
documented functions with nice names and good defaults.

Things we can do :

* ask to document a function in place
* ask to write tests for a functions
* refactor a function
* summarize the latest git commits
* split a script into several scripts

It is built on top chatgpt (default) or llama for now. chatGPT 4o gives impressive results,
The small llama model doesn't perform that well unfortunately and I haven't tried
the bigger ones.

It's in progress and I don't worry too much about breaking things or renaming
functions. But it's definitely already useful, I've actually used it already
to design itself!

## Installation

install with:

``` r
pak::pak("moodymudskipper/ask")
```

You'll also need a chatgpt api key and/or to install llama. 

For the speech to text feature you'll need pythin and might need:

```
brew install portaudio
pip install SpeechRecognition
pip install pyAudio
```

The first one for MacOS only, not sure about other systems, hopefully the 
errors will guide you, use the `ask()` function!

## Simple cases

When we don't providing a `context` argument, the package is a simple interface to the API, with
a system to cache the last result so the use is very comfortable

```{r}
library(ask)

ask("where is the Eiffel tower?")

follow_up("is it high?")

again()
```

## Speech to text

If no input is provided we use speech to text, say "stop listening" to interrupt the
recording, or wait until the time out threshold is reached.

```{r, eval = FALSE}
ask() # ask a question
```

## Using contexts

More interesting however is to ask with a context, context objects are basically
tools to build system messages (directives that sets the context or behavior for the model)
often using data like file content, git history, active script etc...

```{r, eval = FALSE}
# run manually after the README was done
ask("what is this file about, in one sentence?", context = context_script())
# This file is a README script for the `ask` R package that explains its goals, installation process, usage examples, and future development ideas.
ask("what are my last commits about?", context_commits(n = 2))
```

(We could also  have called `ask_script("what is this file about, in one sentence?")` here)

Here are some contexts that you might find useful:

* `context_script()`: Active script by default, but we can provide any path
* `context_repo()`: All R scripts of the repo, README, NAMESPACE, DESCRIPTION,
  LICENSE, LICENSE.md. This might be too much for your LLM context window if you
  have a repo of a decent size.
* `context_session_info()` Basically the output of `SessionInfo()`
*  `context_gmail()` : Email threads, you probably need to restrict `num_results`
* `context_diff()` : Uncommitted changes 
* `context_commits()` : Committed changes

## Update the code base in place

Even more interesting is using those to change your code in place, for this
we use the `ask_in_place()` function.

Try things like :

```{r, eval = FALSE}
ask_in_place("write roxygen2 documentation for functions in the active script", context_script())
ask_in_place("write tests for my_function()", context_script())
ask_in_place("move my_function() to its own script", context_script())
ask_in_place("write a README for this package", context_repo())
```

## Caching system

Caching is useful to spare tokens and because LLMs don't always answer the same thing when asked twice.

There's a `cache` argument to most functions but the recommended use would be to
set `options(ask.cache = "my_cache_folder")`. Then `ask()` will return the same
thing when called with the same prompt and other parameters, calling `again()`
on a conversation triggers a new answer and replaces the cache.

"ram" is a special value to store the cache in memory rather than disk.

## Returning R objects

We have some functions to return objects of given types. If you use cache
you can write reproducible scripts using those.

```{r}
ask_numeric("How many dwarves in Snow White?")

ask_boolean("Do birds sing?")

ask_boolean("Is the Earth flat?")

ask_boolean("Does God exist")

ask_boolean("potatoe")

ask_tibble("Snow White Dwarves + favourite pizza")

ask_tibble("gdps of small countries")
```

## Terminal operations

Because copying and pasting to the terminal is a pain `ask_terminal()` will do it
for you, with commands taking into account your context if you provide it.
It doesn't run those as this would be unsafe, so you'll still have to press Enter.
